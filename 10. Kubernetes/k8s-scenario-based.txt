1. What happens if the k8s master node and worker node firewall gets broken?
-> Existing pods will continue running, but you can't deploy new workloads or updates. API server can't communicate with worker nodes, breaking cluster management capabilities.

2. What happens if etcd backup is corrupted during a cluster restore?
-> Complete cluster failure. The control plane won't initialize and you can't manage resources. Only options are using an older backup or rebuilding the cluster from scratch.

3. What happens if a node's kubelet crashes but the container runtime continues?
-> Existing pods keep running, but the node is marked "NotReady." After grace period (~5min), pods are evicted and rescheduled elsewhere. No new pods will be scheduled to this node.

4. How would applications behave if Kubernetes DNS (CoreDNS) fails?
-> Existing connections work but new service discovery fails. Applications show connection timeouts, and probes using DNS names fail, potentially causing cascading pod restarts.

5. What happens during a network partition between control plane and worker nodes?
-> Worker nodes continue running existing workloads, but controllers can't manage them. New deployments, scaling, and updates fail. Nodes might eventually be marked unreachable.

6. What happens if your CNI plugin starts dropping packets intermittently?
-> Pod-to-pod communication becomes unreliable with sporadic timeouts. Services appear down randomly, causing difficult-to-troubleshoot application failures and inconsistent behavior.

7. What happens to StatefulSets if underlying storage experiences high latency?
-> Pod startup times increase dramatically. Database pods may trigger restart loops, write operations timeout, and new pods get stuck in "ContainerCreating" state.

8. What happens if pod resource limits are set too low?
-> Containers face OOM kills during peak loads. Applications become unstable with unexpected restarts, and horizontal scaling can't solve the problem since each instance is resource-starved.

9. What happens when control plane can't reach cloud provider's API?
-> Cloud-specific features like LoadBalancers and persistent volumes fail. Node initialization is incomplete, and external resource provisioning stops working.

10. What happens if your cluster's certificate authority expires?
-> Complete cluster failure. All component communications fail with TLS errors. API server rejects connections, kubectl stops working, and the entire cluster becomes unusable.

11. What are the Errors you have come across while working on Kubernetes?

Below are Some Known Kuberenets errors:

âœ…ğğ¨ğ ğğğ§ğğ¢ğ§ğ : Pods may remain in the "Pending" state if there are insufficient resources, such as CPU or memory, available on the cluster. Check the resource requests and limits for the pod, and ensure that the node has enough capacity.

âœ…ğ‚ğ«ğšğ¬ğ¡ğ‹ğ¨ğ¨ğ©ğğšğœğ¤ğğŸğŸ: If a pod repeatedly crashes and enters a "CrashLoopBackOff" state, check the container logs using kubectl logs to identify the issue. Common causes include misconfigured application settings or missing dependencies.

âœ…ğˆğ¦ğšğ ğğğ®ğ¥ğ¥ğğšğœğ¤ğğŸğŸ: This error occurs when Kubernetes is unable to pull the container image specified in the pod's manifest. Ensure that the image name and credentials (if needed) are correctly configured. Also, check if there are any issues with the image repository.

âœ…ğğ¨ğğ ğğ¨ğ­ğ‘ğğšğğ²: Nodes in a cluster may become "NotReady" due to various reasons, such as network problems, resource exhaustion, or system issues. Investigate the node's status using kubectl describe node and resolve any underlying problems.

âœ…ğ’ğğ«ğ¯ğ¢ğœğ ğ”ğ§ğšğ¯ğšğ¢ğ¥ğšğ›ğ¥ğ: If a service is not accessible, ensure that the service and its associated pods are running. Use kubectl get pods and kubectl get services to check their statuses.

âœ…ğğğ«ğ¬ğ¢ğ¬ğ­ğğ§ğ­ğ•ğ¨ğ¥ğ®ğ¦ğğ‚ğ¥ğšğ¢ğ¦ (ğğ•ğ‚) ğˆğ¬ğ¬ğ®ğğ¬: Problems with PVCs can lead to pod failures. Check the status of PVCs using kubectl get pvc and make sure they are bound to a PersistentVolume (PV). Also, ensure that the PV is available and in the correct state.

âœ…ğ‘ğğ€ğ‚ ğ€ğ®ğ­ğ¡ğ¨ğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğ„ğ«ğ«ğ¨ğ«ğ¬: If you encounter permission errors, it may be related to Role-Based Access Control (RBAC) settings. Ensure that ServiceAccounts, Roles, and RoleBindings are correctly configured.

âœ…ğğğ­ğ°ğ¨ğ«ğ¤ ğğ¨ğ¥ğ¢ğœğ¢ğğ¬: If you have Network Policies in place, misconfigured policies can lead to network-related issues. Verify that your network policies are correctly defined and applied.

âœ…ğ’ğ­ğ¨ğ«ğšğ ğğ‚ğ¥ğšğ¬ğ¬ ğğ¨ğ­ ğ…ğ¨ğ®ğ§ğ: When using dynamic volume provisioning, make sure that the appropriate StorageClass exists and is accessible. Ensure that the PVC references the correct StorageClass.

âœ…ğ‘ğğ¬ğ¨ğ®ğ«ğœğ ğğ®ğ¨ğ­ğšğ¬ ğšğ§ğ ğ‹ğ¢ğ¦ğ¢ğ­ğ¬: Resource quotas and limits can lead to pods being unable to start or scale. Check the resource limits defined for your namespaces and pods.

âœ…ğˆğ§ğ ğ«ğğ¬ğ¬ ğ‚ğ¨ğ§ğ­ğ«ğ¨ğ¥ğ¥ğğ« ğˆğ¬ğ¬ğ®ğğ¬: If you're using Ingress controllers for routing traffic to services, errors can occur due to misconfigured Ingress resources. Review your Ingress definitions and ensure they match your cluster setup.

You can find good websites/articles for understanding and troubleshooting these errors in more detail on the internet.