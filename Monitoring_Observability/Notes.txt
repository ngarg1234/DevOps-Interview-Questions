Prometheus works on Pull based approach and Push Stragey approach in case of Cron Jobs.....

Questions:: Your development team needs your help to monitor the API endpoint. Which HTTP response would you monitor and when will you trigger the alert....

When you call an API, it responds back with some response code.. if 400 or 500 then you can trigger the alert.....

Observability::

https://github.com/iam-veeramalla/observability-zero-to-hero -- All Notes

Observability tells us the internal state of our system..

It is based on these three pillars:

Metrices- Infra Monitoring, Events, Logs, Traces 

metrices - What -- state of the machine.. tells us the historical data like cpu , memory, disk and https requests..

Events:

Logging --- Why .. tells the logs of why it happended..applications logs....

Traces --- How -- to fix the issue.. extensive information to fix the issue./.


Monitoring vs observability

monitoring oncy covers metrices as well as alerts/dashboards... it is just a subpart of observability..........................

Now we have tools like elk, prometheus to pull the data..

example: checking the heart beat of patient.. check if it is more than 90 alert the nurse if more than 110 alert the doctor for the same.....

https://devopscube.com/prometheus-architecture/

Absolutely! Here's a more detailed yet still concise version of the **Prometheus Architecture** for interview preparation:

---

## üîç Prometheus Architecture ‚Äì Interview Notes

Prometheus is a **time-series-based monitoring and alerting system** that is widely used in cloud-native environments like Kubernetes.
. Its architecture is designed for **scalability**, **modularity**, and **reliability**.

---

### 1. **Prometheus Server**
- **Core component** that:
  - Pulls metrics from targets (scraping).
  - Stores them in its internal **time-series database**.
  - Evaluates rules to trigger alerts.
  - Serves queries via PromQL.
- Scraping happens over HTTP using endpoints like `/metrics`.

---

### 2. **Time-Series Database (TSDB)**
- Prometheus stores data in its **own custom TSDB** on local disk.
- Metrics are stored with **timestamp, metric name, and labels**.
- Supports:
  - **Retention policies**.
  - **Compression** for efficient storage.
  - **Remote Write/Read** integration for external storage systems like Cortex, Thanos, etc.

---

### 3. **Exporters**
- Used to expose metrics from third-party systems that don‚Äôt natively support Prometheus.
- Examples:
  - **Node Exporter** ‚Äì system metrics.
  - **Blackbox Exporter** ‚Äì uptime/probing metrics.
  - **MySQL, Redis, NGINX exporters**, etc.

---

### 4. **Targets**
- Any application or service that exposes metrics in Prometheus format (usually `/metrics` endpoint).
- Prometheus discovers and scrapes these endpoints at regular intervals.

---

### 5. **Push Gateway**
- Used for **short-lived batch jobs** (e.g., cron jobs) that can't be scraped.
- Jobs **push** metrics to the gateway, and Prometheus pulls from it.
- Should not be used for long-lived metrics due to lack of state tracking.

---

### 6. **Alertmanager**
- Handles alerts triggered by Prometheus.
- Features:
  - **Deduplication**, **grouping**, **silencing**, **inhibition**.
  - **Routing** to email, Slack, PagerDuty, etc.
- Prometheus sends alerts based on **alerting rules**.

---

### 7. **PromQL (Prometheus Query Language)**
- A **powerful query language** for slicing, dicing, and aggregating time-series data.
- Used for:
  - Dashboards (Grafana).


  - Alert rules.
  - Ad hoc metric queries.

---

### 8. **Service Discovery**
- Automatically discovers targets for scraping.
- Supports:
  - Static config.
  - Kubernetes.
  - Consul, EC2, GCE, Azure, etc.
- Reduces manual config and adapts to dynamic infrastructure.

---

### 9. **Visualization (Grafana)**
- While Prometheus has a basic UI, **Grafana** is commonly used for rich, interactive dashboards.

---

## ‚úÖ Key Highlights for Interviews
- Pull-based model (except Push Gateway).
- Labels are key for organizing metrics.
- Alertmanager is decoupled from Prometheus server.
- Service discovery enables dynamic environments.
- Scales via remote write/read, federation, or third-party tools like Thanos.

---

Let me know if you want a **flashcard-style** summary or a **diagram** of the architecture too!


# üîç Logging overview
- Logging is crucial in any distributed system, especially in Kubernetes, to monitor application behavior, detect issues, and ensure the smooth functioning of microservices.


## üöÄ Importance:
- **Debugging**: Logs provide critical information when debugging issues in applications.
- **Auditing**: Logs serve as an audit trail, showing what actions were taken and by whom.
- **Performance** Monitoring: Analyzing logs can help identify performance bottlenecks.
- **Security**: Logs help in detecting unauthorized access or malicious activities.

## üõ†Ô∏è Tools Available for Logging in Kubernetes
- üóÇÔ∏è EFK Stack (Elasticsearch, Fluentbit, Kibana)
- üóÇÔ∏è EFK Stack (Elasticsearch, , Kibana)
- üóÇÔ∏è ELK Stack (Elasticsearch, LogstFluentDash, Kibana)
- üìä Promtail + Loki + Grafana

## üì¶ EFK Stack (Elasticsearch, Fluentbit, Kibana)
- EFK is a popular logging stack used to collect, store, and analyze logs in Kubernetes.
- **Elasticsearch**: Stores and indexes log data for easy retrieval.
- **Fluentbit**: A lightweight log forwarder that collects logs from different sources and sends them to Elasticsearch.
- **Kibana**: A visualization tool that allows users to explore and analyze logs stored in Elasticsearch.

# üè† Architecture
![Project Architecture](images/architecture.gif)


## üìù Step-by-Step Setup

### 1) Create IAM Role for Service Account
```bash
eksctl create iamserviceaccount \
    --name ebs-csi-controller-sa \
    --namespace kube-system \
    --cluster observability \
    --role-name AmazonEKS_EBS_CSI_DriverRole \
    --role-only \
    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
    --approve
```
- This command creates an IAM role for the EBS CSI controller.
- IAM role allows EBS CSI controller to interact with AWS resources, specifically for managing EBS volumes in the Kubernetes cluster.
- We will attach the Role with service account

### 2) Retrieve IAM Role ARN
```bash
ARN=$(aws iam get-role --role-name AmazonEKS_EBS_CSI_DriverRole --query 'Role.Arn' --output text)
```
- Command retrieves the ARN of the IAM role created for the EBS CSI controller service account.

### 3) Deploy EBS CSI Driver
```bash
eksctl create addon --cluster observability --name aws-ebs-csi-driver --version latest \
    --service-account-role-arn $ARN --force
```
- Above command deploys the AWS EBS CSI driver as an addon to your Kubernetes cluster.
- It uses the previously created IAM service account role to allow the driver to manage EBS volumes securely.

### 4) Create Namespace for Logging
```bash
kubectl create namespace logging
```

### 5) Install Elasticsearch on K8s

```bash
helm repo add elastic https://helm.elastic.co

helm install elasticsearch \
 --set replicas=1 \
 --set volumeClaimTemplate.storageClassName=gp2 \
 --set persistence.labels.enabled=true elastic/elasticsearch -n logging
```
- Installs Elasticsearch in the `logging` namespace.
- It sets the number of replicas, specifies the storage class, and enables persistence labels to ensure
data is stored on persistent volumes.

### 6) Retrieve Elasticsearch Username & Password
```bash
# for username
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.username}' | base64 -d
# for password
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d
```
- Retrieves the password for the Elasticsearch cluster's master credentials from the Kubernetes secret.
- The password is base64 encoded, so it needs to be decoded before use.
- üëâ **Note**: Please write down the password for future reference

### 7) Install Kibana
```bash
helm install kibana --set service.type=LoadBalancer elastic/kibana -n logging
```
- Kibana provides a user-friendly interface for exploring and visualizing data stored in Elasticsearch.
- It is exposed as a LoadBalancer service, making it accessible from outside the cluster.

### 8) Install Fluentbit with Custom Values/Configurations
- üëâ **Note**: Please update the `HTTP_Passwd` field in the `fluentbit-values.yml` file with the password retrieved earlier in step 6: (i.e NJyO47UqeYBsoaEU)"
```bash
helm repo add fluent https://fluent.github.io/helm-charts
helm install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging
```

## ‚úÖ Conclusion
- We have successfully installed the EFK stack in our Kubernetes cluster, which includes Elasticsearch for storing logs, Fluentbit for collecting and forwarding logs, and Kibana for visualizing logs.
- To verify the setup, access the Kibana dashboard by entering the `LoadBalancer DNS name followed by :5601 in your browser.
    - `http://LOAD_BALANCER_DNS_NAME:5601`
- Use the username and password retrieved in step 6 to log in.
- Once logged in, create a new data view in Kibana and explore the logs collected from your Kubernetes cluster.



## üßº Clean Up
```bash

helm uninstall monitoring -n monitoring

helm uninstall fluent-bit -n logging

helm uninstall elasticsearch -n logging

helm uninstall kibana -n logging

cd day-4

kubectl delete -k kubernetes-manifest/

kubectl delete -k alerts-alertmanager-servicemonitor-manifest/


eksctl delete cluster --name observability

```

 
Finally summary, check in Abhishek link at 3:49:30....PleasePlease

Exporter(HERE Pull) is similar to FluentD,Logstash or Fluentbit(( here PUSH))...pleasepleasepleaseplease 



